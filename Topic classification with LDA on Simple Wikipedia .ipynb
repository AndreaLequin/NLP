{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Creating the corpus of articles from simple Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import codecs\n",
    "import os\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(s) : \n",
    "    return all(ord(c)<128 for c in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=ET.parse('simplewiki/simplewiki-20170201-pages-articles-multistream.xml')\n",
    "root=tree.getroot()\n",
    "url  = '{http://www.mediawiki.org/xml/export-0.10/}page'\n",
    "dir_path_out = 'simplewiki_articles_corpus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_filters(article_txt):\n",
    "    #article_txt = article_txt[ : article_txt.find(\"==\")]\n",
    "    article_txt = re.sub(r\"{{.*}}\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\[\\[File:.*\\]\\]\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\[\\[Image:.*\\]\\]\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\n: \\'\\'.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\n!.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"^:\\'\\'.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"&nbsp\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"http\\S+\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\d+\",\"\",article_txt)   \n",
    "    article_txt = re.sub(r\"\\(.*\\)\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"Category:.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\| .*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\n\\|.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\n \\|.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\".* \\|\\n\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\".*\\|\\n\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{Infobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{infobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{taxobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{Taxobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{ Infobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{ infobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{ taxobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"{{ Taxobox.*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\* .*\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"<.*>\",\"\",article_txt)\n",
    "    article_txt = re.sub(r\"\\n\",\"\",article_txt)  \n",
    "    article_txt = re.sub(r\"\\!|\\\"|\\#|\\$|\\%|\\&|\\'|\\(|\\)|\\*|\\+|\\,|\\-|\\.|\\/|\\:|\\;|\\<|\\=|\\>|\\?|\\@|\\[|\\\\|\\]|\\^|\\_|\\`|\\{|\\||\\}|\\~\",\" \",article_txt)\n",
    "    article_txt = re.sub(r\" +\",\" \",article_txt)\n",
    "    article_txt = article_txt.replace(u'\\xa0', u' ')\n",
    "    return article_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To run only one time : create the corpus\n",
    "\n",
    "#revision_tag=\"{http://www.mediawiki.org/xml/export-0.10/}revision\"\n",
    "#text_tag=\"{http://www.mediawiki.org/xml/export-0.10/}text\"\n",
    "\n",
    "#for i, page in enumerate(root.findall(url)):\n",
    "#    for child_page in page : \n",
    "#        if child_page.tag == revision_tag : \n",
    "#            for x in child_page : \n",
    "#                if x.tag == text_tag : \n",
    "#                    text=x.text\n",
    "#                    if not text == None :\n",
    "#                        text = text[:text.find(\"==\")]\n",
    "#                        article_text=clean_filters(text)\n",
    "#                        if not article_text == None and not article_text == \"\" :\n",
    "#                            if len(article_text) and is_ascii(article_text) :\n",
    "#                                outfile=dir_path_out +str(i+1)+'article.txt'\n",
    "#                                file=codecs.open(outfile,\"w\",\"utf-8\")\n",
    "#                                file.write(article_text)\n",
    "#                                file.close()\n",
    "#                                print(text)\n",
    "#                                print('\\n====================================\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Preprocessing and Training data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal stop word and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/andrealequin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc) :\n",
    "    s_free = \" \".join([i for i in doc.lower().split() if i not in stop])  #list\n",
    "    p_free = \"\".join(w for w in s_free if w not in exclude)               #chain str\n",
    "    lemm = \" \".join(lemma.lemmatize(word) for word in p_free.split())     #list\n",
    "    words = lemm.split()\n",
    "    cleaned = [word for word in words if len(word)>2]\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fich=open(dir_path_out +str(1)+'article.txt',\"r\")\n",
    "art1=fich.read()\n",
    "#art1.lower().split()\n",
    "s1 = \" \".join(i for i in art1.lower().split() if i not in stop)\n",
    "\"\".join(w for w in s1 if w not in exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read contents of all the article in a list doc_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"simplewiki_articles_corpus\"\n",
    "article_paths= [os.path.join(corpus_path, p) for p in os.listdir(corpus_path)]\n",
    "\n",
    "#for mac os : \n",
    "article_paths.remove('simplewiki_articles_corpus/.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_complete=[]\n",
    "for path in article_paths : \n",
    "    fp = codecs.open(path, 'r', 'utf-8')\n",
    "    doc_content=fp.read()\n",
    "    doc_complete.append(doc_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select randomly 70000 articles from the corpus and create a clean train list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_all = random.sample(doc_complete, 70000)\n",
    "docs = open('docs_simplewiki.pkl','wb')\n",
    "pickle.dump(docs_all,docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = docs_all[:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_train_clean = [clean(doc) for doc in docs_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Building word dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_train_clean)\n",
    "\n",
    "# Filter terms which occurs in less than 4 articles & more than 40% of the articles\n",
    "dictionary.filter_extremes(no_below=4, no_above=0.4)\n",
    "\n",
    "# Filter additional stop words : \n",
    "stoplist=set('also use make people know many call include part find become like mean often different \\\n",
    "usually takecome give well get since type list say change see refer actually kinds ask would way \\\n",
    "something need things want every'.split())\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "dictionary.filter_tokens(stop_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Feature Extraction (Bag of Words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_train_clean]\n",
    "#Document term matrix (rows=articles, columns=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) LDA model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel as Lda\n",
    "#building model and training\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=50, id2word=dictionary, passes=50, iterations=500)\n",
    "\n",
    "#dump LDA model using pickle\n",
    "ldafile = open('lda_model_sym_simplewiki', 'wb')\n",
    "pickle.dump(ldamodel, ldafile)\n",
    "ldafile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.088*\"british\" ', ' 0.079*\"class\" ', ' 0.053*\"unit\" ', ' 0.044*\"rail\" ', ' 0.039*\"service\" ', ' 0.025*\"australian\" ', ' 0.023*\"electric\" ', ' 0.021*\"website\" ', ' 0.016*\"multiple\" ', ' 0.015*\"locomotive\"'] \n",
      "\n",
      "['0.053*\"chemical\" ', ' 0.046*\"company\" ', ' 0.029*\"contains\" ', ' 0.027*\"number\" ', ' 0.026*\"studio\" ', ' 0.022*\"compound\" ', ' 0.021*\"sold\" ', ' 0.019*\"made\" ', ' 0.019*\"disney\" ', ' 0.019*\"acid\"'] \n",
      "\n",
      "['0.061*\"church\" ', ' 0.038*\"saint\" ', ' 0.026*\"guitar\" ', ' 0.024*\"doctor\" ', ' 0.024*\"pay\" ', ' 0.022*\"southwest\" ', ' 0.020*\"catholic\" ', ' 0.020*\"wwe\" ', ' 0.020*\"wrestling\" ', ' 0.019*\"gironde\"'] \n",
      "\n",
      "['0.041*\"food\" ', ' 0.041*\"hot\" ', ' 0.034*\"culture\" ', ' 0.025*\"flag\" ', ' 0.025*\"england\" ', ' 0.021*\"oil\" ', ' 0.021*\"fox\" ', ' 0.018*\"heat\" ', ' 0.018*\"ruler\" ', ' 0.017*\"greater\"'] \n",
      "\n",
      "['0.038*\"web\" ', ' 0.037*\"earth\" ', ' 0.034*\"cite\" ', ' 0.030*\"period\" ', ' 0.030*\"white\" ', ' 0.027*\"black\" ', ' 0.019*\"middle\" ', ' 0.018*\"mass\" ', ' 0.016*\"planet\" ', ' 0.015*\"birth\"'] \n",
      "\n",
      "['0.061*\"computer\" ', ' 0.058*\"number\" ', ' 0.051*\"system\" ', ' 0.036*\"request\" ', ' 0.029*\"data\" ', ' 0.028*\"information\" ', ' 0.027*\"text\" ', ' 0.024*\"program\" ', ' 0.021*\"code\" ', ' 0.018*\"wikipedia\"'] \n",
      "\n",
      "['0.705*\"redirect\" ', ' 0.042*\"template\" ', ' 0.036*\"switzerland\" ', ' 0.030*\"canton\" ', ' 0.029*\"municipality\" ', ' 0.014*\"district\" ', ' 0.011*\"data\" ', ' 0.007*\"mayor\" ', ' 0.007*\"internet\" ', ' 0.006*\"domain\"'] \n",
      "\n",
      "['0.129*\"city\" ', ' 0.062*\"town\" ', ' 0.046*\"capital\" ', ' 0.042*\"area\" ', ' 0.037*\"population\" ', ' 0.027*\"largest\" ', ' 0.023*\"live\" ', ' 0.020*\"site\" ', ' 0.020*\"census\" ', ' 0.015*\"square\"'] \n",
      "\n",
      "['0.072*\"year\" ', ' 0.034*\"common\" ', ' 0.026*\"novel\" ', ' 0.026*\"heart\" ', ' 0.025*\"radio\" ', ' 0.023*\"writing\" ', ' 0.022*\"son\" ', ' 0.022*\"calendar\" ', ' 0.021*\"starting\" ', ' 0.020*\"netherlands\"'] \n",
      "\n",
      "['0.108*\"england\" ', ' 0.089*\"club\" ', ' 0.073*\"italy\" ', ' 0.046*\"sport\" ', ' 0.045*\"english\" ', ' 0.042*\"stadium\" ', ' 0.042*\"italian\" ', ' 0.031*\"james\" ', ' 0.028*\"chicago\" ', ' 0.026*\"charles\"'] \n",
      "\n",
      "['0.039*\"king\" ', ' 0.029*\"first\" ', ' 0.020*\"became\" ', ' 0.017*\"june\" ', ' 0.016*\"election\" ', ' 0.016*\"august\" ', ' 0.015*\"went\" ', ' 0.015*\"november\" ', ' 0.015*\"started\" ', ' 0.014*\"david\"'] \n",
      "\n",
      "['0.047*\"time\" ', ' 0.038*\"point\" ', ' 0.034*\"example\" ', ' 0.033*\"term\" ', ' 0.029*\"special\" ', ' 0.026*\"process\" ', ' 0.026*\"energy\" ', ' 0.024*\"must\" ', ' 0.023*\"project\" ', ' 0.016*\"amount\"'] \n",
      "\n",
      "['0.051*\"kingdom\" ', ' 0.039*\"government\" ', ' 0.039*\"party\" ', ' 0.034*\"united\" ', ' 0.027*\"british\" ', ' 0.024*\"president\" ', ' 0.022*\"scotland\" ', ' 0.020*\"queen\" ', ' 0.019*\"minister\" ', ' 0.019*\"ireland\"'] \n",
      "\n",
      "['0.176*\"movie\" ', ' 0.127*\"american\" ', ' 0.057*\"award\" ', ' 0.041*\"star\" ', ' 0.034*\"actor\" ', ' 0.026*\"director\" ', ' 0.023*\"baseball\" ', ' 0.022*\"los\" ', ' 0.021*\"directed\" ', ' 0.021*\"writer\"'] \n",
      "\n",
      "['0.086*\"world\" ', ' 0.066*\"group\" ', ' 0.059*\"one\" ', ' 0.040*\"four\" ', ' 0.039*\"three\" ', ' 0.035*\"two\" ', ' 0.028*\"five\" ', ' 0.023*\"member\" ', ' 0.020*\"together\" ', ' 0.019*\"seven\"'] \n",
      "\n",
      "['0.091*\"character\" ', ' 0.043*\"network\" ', ' 0.035*\"house\" ', ' 0.029*\"medium\" ', ' 0.029*\"fiction\" ', ' 0.028*\"foot\" ', ' 0.026*\"fictional\" ', ' 0.019*\"appeared\" ', ' 0.018*\"head\" ', ' 0.018*\"suffolk\"'] \n",
      "\n",
      "['0.038*\"geobox\" ', ' 0.029*\"million\" ', ' 0.026*\"style\" ', ' 0.020*\"hard\" ', ' 0.018*\"making\" ', ' 0.017*\"across\" ', ' 0.017*\"mainly\" ', ' 0.015*\"damage\" ', ' 0.015*\"true\" ', ' 0.015*\"inside\"'] \n",
      "\n",
      "['0.100*\"station\" ', ' 0.095*\"london\" ', ' 0.083*\"line\" ', ' 0.059*\"railway\" ', ' 0.039*\"george\" ', ' 0.031*\"bridge\" ', ' 0.030*\"street\" ', ' 0.025*\"john\" ', ' 0.025*\"illinois\" ', ' 0.023*\"bank\"'] \n",
      "\n",
      "['0.080*\"right\" ', ' 0.056*\"following\" ', ' 0.053*\"woman\" ', ' 0.044*\"help\" ', ' 0.041*\"law\" ', ' 0.036*\"archive\" ', ' 0.035*\"talk\" ', ' 0.034*\"brazil\" ', ' 0.032*\"discussion\" ', ' 0.029*\"preserved\"'] \n",
      "\n",
      "['0.066*\"living\" ', ' 0.058*\"civil\" ', ' 0.054*\"village\" ', ' 0.050*\"blue\" ', ' 0.044*\"england\" ', ' 0.041*\"olympic\" ', ' 0.036*\"parish\" ', ' 0.031*\"summer\" ', ' 0.030*\"beach\" ', ' 0.027*\"wave\"'] \n",
      "\n",
      "['0.129*\"hurricane\" ', ' 0.114*\"season\" ', ' 0.085*\"bar\" ', ' 0.057*\"button\" ', ' 0.051*\"god\" ', ' 0.033*\"washington\" ', ' 0.032*\"road\" ', ' 0.031*\"start\" ', ' 0.020*\"market\" ', ' 0.016*\"cut\"'] \n",
      "\n",
      "['0.090*\"district\" ', ' 0.077*\"pakistan\" ', ' 0.046*\"disambig\" ', ' 0.044*\"key\" ', ' 0.041*\"atlantic\" ', ' 0.041*\"storm\" ', ' 0.035*\"tropical\" ', ' 0.031*\"union\" ', ' 0.029*\"council\" ', ' 0.025*\"box\"'] \n",
      "\n",
      "['0.046*\"league\" ', ' 0.037*\"team\" ', ' 0.028*\"canada\" ', ' 0.027*\"hockey\" ', ' 0.025*\"played\" ', ' 0.023*\"championship\" ', ' 0.023*\"season\" ', ' 0.022*\"american\" ', ' 0.020*\"ice\" ', ' 0.020*\"canadian\"'] \n",
      "\n",
      "['0.123*\"university\" ', ' 0.063*\"school\" ', ' 0.048*\"age\" ', ' 0.044*\"college\" ', ' 0.036*\"roman\" ', ' 0.031*\"public\" ', ' 0.028*\"student\" ', ' 0.026*\"high\" ', ' 0.021*\"degree\" ', ' 0.017*\"education\"'] \n",
      "\n",
      "['0.119*\"century\" ', ' 0.091*\"year\" ', ' 0.076*\"end\" ', ' 0.057*\"related\" ', ' 0.050*\"event\" ', ' 0.024*\"considered\" ', ' 0.023*\"route\" ', ' 0.023*\"begin\" ', ' 0.022*\"interstate\" ', ' 0.021*\"martin\"'] \n",
      "\n",
      "['0.130*\"river\" ', ' 0.084*\"north\" ', ' 0.078*\"south\" ', ' 0.077*\"island\" ', ' 0.055*\"east\" ', ' 0.054*\"west\" ', ' 0.044*\"australia\" ', ' 0.035*\"northern\" ', ' 0.033*\"park\" ', ' 0.027*\"near\"'] \n",
      "\n",
      "['0.069*\"used\" ', ' 0.034*\"called\" ', ' 0.024*\"thing\" ', ' 0.023*\"made\" ', ' 0.022*\"one\" ', ' 0.019*\"wikt\" ', ' 0.018*\"sometimes\" ', ' 0.017*\"may\" ', ' 0.017*\"body\" ', ' 0.016*\"kind\"'] \n",
      "\n",
      "['0.045*\"province\" ', ' 0.043*\"science\" ', ' 0.033*\"study\" ', ' 0.027*\"theory\" ', ' 0.025*\"case\" ', ' 0.024*\"model\" ', ' 0.022*\"natural\" ', ' 0.022*\"structure\" ', ' 0.021*\"field\" ', ' 0.019*\"think\"'] \n",
      "\n",
      "['0.321*\"state\" ', ' 0.229*\"united\" ', ' 0.118*\"county\" ', ' 0.078*\"city\" ', ' 0.021*\"seat\" ', ' 0.019*\"iowa\" ', ' 0.016*\"american\" ', ' 0.013*\"virginia\" ', ' 0.010*\"carolina\" ', ' 0.008*\"georgia\"'] \n",
      "\n",
      "['0.245*\"france\" ', ' 0.141*\"region\" ', ' 0.119*\"department\" ', ' 0.117*\"commune\" ', ' 0.066*\"found\" ', ' 0.031*\"north\" ', ' 0.030*\"calais\" ', ' 0.025*\"calvados\" ', ' 0.019*\"northwest\" ', ' 0.015*\"nord\"'] \n",
      "\n",
      "['0.038*\"history\" ', ' 0.025*\"empire\" ', ' 0.024*\"religion\" ', ' 0.020*\"ancient\" ', ' 0.020*\"territory\" ', ' 0.020*\"group\" ', ' 0.018*\"community\" ', ' 0.017*\"half\" ', ' 0.017*\"table\" ', ' 0.016*\"prize\"'] \n",
      "\n",
      "['0.083*\"may\" ', ' 0.073*\"person\" ', ' 0.039*\"disease\" ', ' 0.035*\"cause\" ', ' 0.033*\"someone\" ', ' 0.030*\"drug\" ', ' 0.030*\"navbox\" ', ' 0.028*\"strong\" ', ' 0.027*\"atom\" ', ' 0.027*\"blood\"'] \n",
      "\n",
      "['0.036*\"building\" ', ' 0.030*\"florida\" ', ' 0.025*\"built\" ', ' 0.023*\"lived\" ', ' 0.023*\"year\" ', ' 0.022*\"division\" ', ' 0.022*\"china\" ', ' 0.020*\"mountain\" ', ' 0.018*\"hill\" ', ' 0.018*\"land\"'] \n",
      "\n",
      "['0.086*\"game\" ', ' 0.068*\"series\" ', ' 0.053*\"television\" ', ' 0.045*\"show\" ', ' 0.032*\"video\" ', ' 0.023*\"best\" ', ' 0.020*\"released\" ', ' 0.019*\"first\" ', ' 0.018*\"made\" ', ' 0.018*\"film\"'] \n",
      "\n",
      "['0.237*\"noinclude\" ', ' 0.196*\"new\" ', ' 0.085*\"germany\" ', ' 0.072*\"york\" ', ' 0.056*\"german\" ', ' 0.030*\"national\" ', ' 0.020*\"wisconsin\" ', ' 0.018*\"austria\" ', ' 0.018*\"green\" ', ' 0.016*\"mount\"'] \n",
      "\n",
      "['0.100*\"book\" ', ' 0.048*\"story\" ', ' 0.040*\"written\" ', ' 0.030*\"color\" ', ' 0.028*\"first\" ', ' 0.027*\"alabama\" ', ' 0.024*\"wrote\" ', ' 0.022*\"short\" ', ' 0.021*\"published\" ', ' 0.019*\"letter\"'] \n",
      "\n",
      "['0.055*\"day\" ', ' 0.037*\"action\" ', ' 0.037*\"oklahoma\" ', ' 0.027*\"last\" ', ' 0.027*\"friend\" ', ' 0.024*\"feature\" ', ' 0.022*\"signal\" ', ' 0.022*\"active\" ', ' 0.021*\"carry\" ', ' 0.021*\"animation\"'] \n",
      "\n",
      "['0.213*\"category\" ', ' 0.155*\"article\" ', ' 0.063*\"california\" ', ' 0.059*\"san\" ', ' 0.055*\"european\" ', ' 0.053*\"general\" ', ' 0.038*\"victoria\" ', ' 0.034*\"melbourne\" ', ' 0.033*\"title\" ', ' 0.031*\"grand\"'] \n",
      "\n",
      "['0.187*\"language\" ', ' 0.154*\"name\" ', ' 0.085*\"english\" ', ' 0.023*\"official\" ', ' 0.023*\"latin\" ', ' 0.021*\"southern\" ', ' 0.018*\"given\" ', ' 0.017*\"generation\" ', ' 0.015*\"first\" ', ' 0.015*\"figure\"'] \n",
      "\n",
      "['0.083*\"music\" ', ' 0.065*\"album\" ', ' 0.059*\"band\" ', ' 0.058*\"song\" ', ' 0.050*\"rock\" ', ' 0.039*\"released\" ', ' 0.029*\"single\" ', ' 0.027*\"record\" ', ' 0.025*\"singer\" ', ' 0.018*\"american\"'] \n",
      "\n",
      "['0.054*\"word\" ', ' 0.043*\"war\" ', ' 0.031*\"good\" ', ' 0.023*\"force\" ', ' 0.022*\"greek\" ', ' 0.019*\"world\" ', ' 0.018*\"military\" ', ' 0.018*\"side\" ', ' 0.017*\"meaning\" ', ' 0.017*\"used\"'] \n",
      "\n",
      "['0.054*\"family\" ', ' 0.047*\"plant\" ', ' 0.047*\"specie\" ', ' 0.037*\"animal\" ', ' 0.031*\"europe\" ', ' 0.030*\"america\" ', ' 0.025*\"red\" ', ' 0.024*\"tree\" ', ' 0.023*\"africa\" ', ' 0.022*\"order\"'] \n",
      "\n",
      "['0.064*\"water\" ', ' 0.034*\"sea\" ', ' 0.031*\"lake\" ', ' 0.030*\"software\" ', ' 0.027*\"ocean\" ', ' 0.026*\"run\" ', ' 0.025*\"free\" ', ' 0.025*\"open\" ', ' 0.022*\"fish\" ', ' 0.020*\"bay\"'] \n",
      "\n",
      "['0.047*\"light\" ', ' 0.044*\"car\" ', ' 0.040*\"one\" ', ' 0.032*\"element\" ', ' 0.023*\"race\" ', ' 0.023*\"gas\" ', ' 0.022*\"sun\" ', ' 0.019*\"racing\" ', ' 0.019*\"nature\" ', ' 0.018*\"formula\"'] \n",
      "\n",
      "['0.263*\"football\" ', ' 0.154*\"player\" ', ' 0.083*\"national\" ', ' 0.083*\"team\" ', ' 0.076*\"association\" ', ' 0.067*\"infobox\" ', ' 0.059*\"japan\" ', ' 0.040*\"play\" ', ' 0.037*\"ese\" ', ' 0.035*\"played\"'] \n",
      "\n",
      "['0.186*\"country\" ', ' 0.060*\"india\" ', ' 0.059*\"republic\" ', ' 0.028*\"indian\" ', ' 0.027*\"organization\" ', ' 0.023*\"korea\" ', ' 0.021*\"international\" ', ' 0.020*\"wind\" ', ' 0.018*\"trade\" ', ' 0.016*\"usa\"'] \n",
      "\n",
      "['0.059*\"power\" ', ' 0.034*\"movement\" ', ' 0.033*\"dog\" ', ' 0.030*\"spain\" ', ' 0.027*\"society\" ', ' 0.023*\"spanish\" ', ' 0.021*\"health\" ', ' 0.018*\"african\" ', ' 0.017*\"among\" ', ' 0.016*\"zealand\"'] \n",
      "\n",
      "['0.135*\"wikipedia\" ', ' 0.060*\"simple\" ', ' 0.057*\"link\" ', ' 0.048*\"airport\" ', ' 0.046*\"international\" ', ' 0.027*\"centre\" ', ' 0.026*\"texas\" ', ' 0.024*\"airline\" ', ' 0.023*\"colour\" ', ' 0.019*\"arkansas\"'] \n",
      "\n",
      "['0.092*\"page\" ', ' 0.080*\"template\" ', ' 0.061*\"user\" ', ' 0.046*\"image\" ', ' 0.046*\"stub\" ', ' 0.031*\"file\" ', ' 0.027*\"internet\" ', ' 0.019*\"little\" ', ' 0.019*\"standard\" ', ' 0.019*\"display\"'] \n",
      "\n",
      "['0.025*\"life\" ', ' 0.024*\"born\" ', ' 0.020*\"art\" ', ' 0.020*\"famous\" ', ' 0.020*\"death\" ', ' 0.019*\"year\" ', ' 0.019*\"died\" ', ' 0.018*\"known\" ', ' 0.018*\"child\" ', ' 0.017*\"time\"'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print all the 50 topics\n",
    "for i, topics in enumerate(ldamodel.print_topics(num_topics=50, num_words=10)):\n",
    "    words=topics[1].split(\"+\")\n",
    "    print (words, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
