{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Activation, Reshape\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(article):\n",
    "    stopword_list = set(stopwords.words(\"english\"))\n",
    "    return [word for word in article.split() if word not in stopword_list]\n",
    "\n",
    "\n",
    "def remove_character(article):\n",
    "    return re.sub(\"[^a-zA-Z]+\", \" \", article)\n",
    "\n",
    "def remove_one_character_token(article) : \n",
    "    return [word for word in article if len(word)>1]\n",
    "    \n",
    "def lower(string):\n",
    "    return string.lower()\n",
    "\n",
    "\n",
    "def clean(article):\n",
    "    article = article.strip()\n",
    "    article = lower(article)\n",
    "    article = remove_character(article)\n",
    "    article = remove_stopwords(article)\n",
    "    article = remove_one_character_token(article)\n",
    "    article = ' '.join(article)\n",
    "    return article\n",
    "\n",
    "def clean2(article):\n",
    "    #article = article.strip()\n",
    "    article = lower(article)\n",
    "    article = remove_character(article)\n",
    "    article = remove_stopwords(article)\n",
    "    article = remove_one_character_token(article)\n",
    "    article = ' '.join(article)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_count_vec(max_n_word=20000,mode_tfidf = True) : \n",
    "    \n",
    "    text_train = []\n",
    "    target_train = []\n",
    "    text_val = []\n",
    "    target_val = []\n",
    "    text_test = []\n",
    "    target_test = []   \n",
    "    \n",
    "    print('Loading and cleaning files ...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    for link in glob.glob('/Users/andrealequin/Desktop/Stage_NLP/Simple_extraction/train_list/*.txt') : \n",
    "        file = open(link,'r')\n",
    "        text_train.append(clean(file.read()))\n",
    "        target_train.append(int(os.path.basename(file.name)[0:2]))\n",
    "\n",
    "        \n",
    "    for link in glob.glob('/Users/andrealequin/Desktop/Stage_NLP/Simple_extraction/val_list/*.txt') : \n",
    "        file = open(link,'r')\n",
    "        text_val.append(clean(file.read()))\n",
    "        target_val.append(int(os.path.basename(file.name)[0:2]))   \n",
    "\n",
    "    for link in glob.glob('/Users/andrealequin/Desktop/Stage_NLP/Simple_extraction/test_list/*.txt') : \n",
    "        file = open(link,'r')\n",
    "        text_test.append(clean(file.read()))\n",
    "        target_test.append(int(os.path.basename(file.name)[0:2]))   \n",
    "        \n",
    "    t1 = time.time()\n",
    "    \n",
    "    print('Loading and cleaning files time : ','%.1f' % (t1-t0),'s')\n",
    "    \n",
    "    print('Count Vectorization ...')\n",
    "    \n",
    "    if mode_tfidf == True : \n",
    "        vectorizer = TfidfVectorizer(max_features = max_n_word)\n",
    "        print('mode : tdidf')\n",
    "    else : \n",
    "        vectorizer = CountVectorizer(max_features = max_n_word)\n",
    "        print('mode : simple count')\n",
    "        \n",
    "    x_train = vectorizer.fit_transform(text_train)\n",
    "    y_train = np.asarray(target_train)\n",
    "  \n",
    "    x_val = vectorizer.transform(text_val)\n",
    "    y_val = np.asarray(target_val)\n",
    "    \n",
    "    x_test = vectorizer.transform(text_test)\n",
    "    y_test = np.asarray(target_test)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print('Vectorization time : ', '%.1f' % (t2-t1),'s')\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test, text_train, text_val, text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences_input() : \n",
    "    t0 = time.time()\n",
    "    print('Creating tokenized sentences to feed Word2Vec model ...')\n",
    "    list_of_sentences = []\n",
    "    cnt = 0\n",
    "    for link in glob.glob('/Users/andrealequin/Desktop/Stage_NLP/Simple_extraction/train_list/*.txt') : \n",
    "        file = open(link,'r')\n",
    "        article = file.read()\n",
    "        split_art= re.split('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s',article) #split into sentences\n",
    "        for sentence in split_art :\n",
    "            clean_sentence = clean2(sentence)\n",
    "            sentence_into_list = clean_sentence.split() \n",
    "            list_of_sentences.append(sentence_into_list)\n",
    "        #cnt+=1AndrÃ©a Lequin\n",
    "        #if cnt>10 : \n",
    "        #    break\n",
    "    t1 = time.time()\n",
    "    print('Splitting and cleaning time : ', '%.1f' % (t1-t0),'s')\n",
    "    return list_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning files ...\n",
      "Loading and cleaning files time :  45.2 s\n",
      "Count Vectorization ...\n",
      "mode : simple count\n",
      "Vectorization time :  21.5 s\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, y_test, text_train, text_val, text_test = create_input_count_vec(max_n_word=20000,mode_tfidf = False)\n",
    "#sets = x_train, y_train, x_val, y_val, x_test, y_test, text_train, text_val, text_test\n",
    "sets2 = x_train, y_train, x_val, y_val, x_test, y_test, text_train, text_val, text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tokenized sentences to feed Word2Vec model ...\n",
      "Splitting and cleaning time :  318.2 s\n"
     ]
    }
   ],
   "source": [
    "list_sentences_train = create_sentences_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['knight', 'tour', 'sequence', 'moves', 'knight', 'chessboard', 'knight', 'visits', 'every', 'square', 'exactly'], ['knight', 'ends', 'square', 'one', 'knight', 'move', 'beginning', 'square', 'could', 'tour', 'board', 'immediately', 'following', 'path', 'tour', 'closed', 'otherwise', 'open', 'knight', 'tour', 'problem', 'mathematical', 'problem', 'finding', 'knight', 'tour'], ['creating', 'program', 'find', 'knight', 'tour', 'common', 'problem', 'given', 'computer', 'science', 'students'], ['variations', 'knight', 'tour', 'problem', 'involve', 'chessboards', 'different', 'sizes', 'usual', 'well', 'irregular', 'non', 'rectangular', 'boards'], ['theory', 'knight', 'tour', 'problem', 'instance', 'general', 'hamiltonian', 'path', 'problem', 'graph', 'theory'], ['problem', 'finding', 'closed', 'knight', 'tour', 'similarly', 'instance', 'hamiltonian', 'cycle', 'problem'], ['unlike', 'general', 'hamiltonian', 'path', 'problem', 'knight', 'tour', 'problem', 'solved', 'linear', 'time'], ['history', 'earliest', 'known', 'reference', 'knight', 'tour', 'problem', 'dates', 'back', 'th', 'century', 'ad'], ['rudra', 'kavyalankara', 'sanskrit', 'work', 'poetics', 'pattern', 'knight', 'tour', 'half', 'board', 'presented', 'elaborate', 'poetic', 'figure', 'citra', 'ala', 'ra', 'called', 'turagapadabandha', 'arrangement', 'steps', 'horse'], ['verse', 'four', 'lines', 'eight', 'syllables', 'read', 'left', 'right', 'following', 'path', 'knight', 'tour']]\n"
     ]
    }
   ],
   "source": [
    "print(list_sentences_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logreg(sets) :\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test, text_train, text_val, text_test = sets\n",
    "    print('Build logistic regression model ...')\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_train)\n",
    "    acc = accuracy_score(y_train, y_pred)\n",
    "    y_pred = clf.predict(x_val)\n",
    "    val_acc = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    print('acc :', acc)\n",
    "    print('val acc :', val_acc)\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "    \n",
    "    return #clf.predict_proba(np.concatenate((x_train, x_val), axis=0))[:, 1], \\\n",
    "           #clf.predict_proba(x_test)[:, 1], \\\n",
    "           #np.concatenate((y_train, y_val), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build logistic regression model ...\n",
      "acc : 0.9295279912184413\n",
      "val acc : 0.857703081232493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92       400\n",
      "           1       0.88      0.92      0.90       400\n",
      "           2       0.88      0.79      0.83       185\n",
      "           3       0.82      0.86      0.84       400\n",
      "           4       0.79      0.79      0.79       400\n",
      "\n",
      "    accuracy                           0.86      1785\n",
      "   macro avg       0.86      0.85      0.85      1785\n",
      "weighted avg       0.86      0.86      0.86      1785\n",
      "\n",
      "[[359   4   0  19  18]\n",
      " [  3 369   1   9  18]\n",
      " [  1   2 146  20  16]\n",
      " [  7   4  12 343  34]\n",
      " [ 13  40   7  26 314]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrealequin/anaconda/envs/envtf/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "run_logreg(sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build logistic regression model ...\n",
      "acc : 0.9670691547749726\n",
      "val acc : 0.8140056022408964\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90       400\n",
      "           1       0.84      0.87      0.86       400\n",
      "           2       0.70      0.75      0.72       185\n",
      "           3       0.79      0.81      0.80       400\n",
      "           4       0.76      0.70      0.73       400\n",
      "\n",
      "    accuracy                           0.81      1785\n",
      "   macro avg       0.80      0.81      0.80      1785\n",
      "weighted avg       0.81      0.81      0.81      1785\n",
      "\n",
      "[[361   5   3  16  15]\n",
      " [  3 348   7  11  31]\n",
      " [  0   8 138  19  20]\n",
      " [ 15   8  32 324  21]\n",
      " [ 19  44  16  39 282]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrealequin/anaconda/envs/envtf/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "run_logreg(sets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(sets): \n",
    "    x_train, y_train, x_val, y_val, x_test, y_test, text_train, text_val, text_test = sets\n",
    "    print('Build SVM model ...')\n",
    "    #[x_train, y_train, x_val, y_val, x_test, y_test] = pickle.load(open('/Users/andrealequin/Desktop/Stage_NLP/Simple_extraction/dumps/'+str(num_set)+'.dat', \"rb\"))\n",
    "    svm = LinearSVC(C=0.0001)\n",
    "    svm = CalibratedClassifierCV(svm)\n",
    "    svm.fit(x_train, y_train)\n",
    "    y_pred = svm.predict(x_train)\n",
    "    acc = accuracy_score(y_train, y_pred)\n",
    "    y_pred = svm.predict(x_val)\n",
    "    val_acc = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    print('acc :', acc)\n",
    "    print('val acc :', val_acc)\n",
    "    print(y_pred)\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "    return #svm.predict_proba(np.concatenate((x_train, x_val), axis=0))[:, 1], \\\n",
    "           #svm.predict_proba(x_test)[:, 1], \\\n",
    "           #np.concatenate((y_train, y_val), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build SVM model ...\n",
      "acc : 0.8384193194291987\n",
      "val acc : 0.8212885154061624\n",
      "[4 4 4 ... 3 3 3]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88       400\n",
      "           1       0.84      0.91      0.87       400\n",
      "           2       0.82      0.77      0.79       185\n",
      "           3       0.81      0.80      0.80       400\n",
      "           4       0.71      0.77      0.74       400\n",
      "\n",
      "    accuracy                           0.82      1785\n",
      "   macro avg       0.83      0.82      0.82      1785\n",
      "weighted avg       0.83      0.82      0.82      1785\n",
      "\n",
      "[[334   5   1  28  32]\n",
      " [  3 362   2   9  24]\n",
      " [  1   3 142  20  19]\n",
      " [  4   8  19 321  48]\n",
      " [ 13  51   9  20 307]]\n"
     ]
    }
   ],
   "source": [
    "run_svm(sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RandomForest(sets):\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test, text_train, text_val, text_test = sets\n",
    "    print('Build Random Forest model ...')\n",
    "    #[x_train, y_train, x_val, y_val, x_test, y_test] = pickle.load(open('/Users/andrealequin/Desktop/Stage_NLP/Simple_extraction/dumps/'+str(num_set)+'.dat', \"rb\"))\n",
    "    clf = RandomForestClassifier(max_depth=3, random_state=0)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_train)\n",
    "    acc = accuracy_score(y_train, y_pred)\n",
    "    y_pred = clf.predict(x_val)\n",
    "    val_acc = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    print('acc :', acc)\n",
    "    print('val acc :', val_acc)\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "    \n",
    "    return #clf.predict_proba(np.concatenate((x_train, x_val), axis=0))[:, 1], \\\n",
    "           #clf.predict_proba(x_test)[:, 1], \\\n",
    "           #np.concatenate((y_train, y_val), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Random Forest model ...\n",
      "acc : 0.7054884742041713\n",
      "val acc : 0.6868347338935574\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.87       400\n",
      "           1       0.75      0.89      0.82       400\n",
      "           2       1.00      0.01      0.01       185\n",
      "           3       0.55      0.80      0.65       400\n",
      "           4       0.61      0.51      0.56       400\n",
      "\n",
      "    accuracy                           0.69      1785\n",
      "   macro avg       0.76      0.61      0.58      1785\n",
      "weighted avg       0.73      0.69      0.65      1785\n",
      "\n",
      "[[345  10   0  29  16]\n",
      " [  3 355   0  30  12]\n",
      " [  4   5   1 116  59]\n",
      " [ 21  14   0 320  45]\n",
      " [ 24  87   0  84 205]]\n"
     ]
    }
   ],
   "source": [
    "run_RandomForest(sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text to sequences Train/Val/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 50000\n",
    "#MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 254711 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower= True)\n",
    "#tokenizer.fit_on_texts(text_train+text_val+text_test)\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(text_train)\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_val = tokenizer.texts_to_sequences(text_val)\n",
    "X_val = pad_sequences(X_val, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = tokenizer.texts_to_sequences(text_test)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.get_dummies(y_train).values\n",
    "Y_val = pd.get_dummies(y_val).values\n",
    "Y_test = pd.get_dummies(y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9110, 250) (9110, 5)\n",
      "(1785, 250) (1785, 5)\n",
      "(1785, 250) (1785, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_val.shape,Y_val.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 1]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]]\n",
      "[[  266    35 19509 ...  1952 16812   627]\n",
      " [ 1407   198  3337 ...  3976    20  1474]\n",
      " [ 5768 11678  2751 ...  4485  2822   272]\n",
      " ...\n",
      " [ 4764 47264   304 ...  4689   592   183]\n",
      " [    0     0     0 ...   281  2659   376]\n",
      " [  582  1525 17734 ...   235   668   244]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[:10])\n",
    "print(X_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model 1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 255       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 5,030,455\n",
      "Trainable params: 5,030,455\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.name=\"Model 1\"\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "#model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model 2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 250, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 250, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 5,080,905\n",
      "Trainable params: 5,080,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.name=\"Model 2\"\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model 3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 250, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 250, 20)           8880      \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 20)                2480      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 105       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 5,011,465\n",
      "Trainable params: 5,011,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.name=\"Model 3\"\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))\n",
    "model.add(Bidirectional(LSTM(10)))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model 4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 250, 100)          20000000  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 250, 64)           6464      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 80005     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,086,469\n",
      "Trainable params: 20,086,469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.name=\"Model 4\"\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(64))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mettre un predict\n",
    "#verifier meme taille que les batch + positif entre 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model 5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 250, 50)           2500000   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 250, 55)           2805      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 125, 55)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 6875)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 250)               1719000   \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 4,223,060\n",
      "Trainable params: 4,223,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.name=\"Model 5\"\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(Convolution1D(filters=55, kernel_size=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "#opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "opt = 'adam'\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model 6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 250, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 250, 32)           3232      \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 5)                 40005     \n",
      "=================================================================\n",
      "Total params: 5,043,237\n",
      "Trainable params: 5,043,237\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.name=\"Model 6\"\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "#opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "optimizer = 'adam'\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model 7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 250, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 250, 32)           3232      \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 5)                 40005     \n",
      "=================================================================\n",
      "Total params: 5,043,237\n",
      "Trainable params: 5,043,237\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.name=\"Model 7\"\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(Dense(32, activation='relu',kernel_regularizer=l2(0.1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "#opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "optimizer = 'adam'\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Word2vec on the train corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9110"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Word2Vec Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrealequin/anaconda/envs/envtf/lib/python3.7/site-packages/gensim/models/base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "print('Building Word2Vec Model ...')\n",
    "model = Word2Vec(list_sentences_train, min_count=5,workers=50)\n",
    "t1 = time.time()\n",
    "print('Time for building Word2Vec model : ', '%.1f' % (t1-t0),'s')\n",
    "print(model)\n",
    "words = list(model.wv.vocab)\n",
    "#print(words)\n",
    "print(model['computer'])\n",
    "model.save('model.bin')\n",
    "#new_model = Word2Vec.load('model.bin')\n",
    "#print(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Google Word2vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Stanford Glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254711\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "\n",
    "e = Embedding(MAX_NB_WORDS, 100, weights=[embedding_matrix], input_length=X_train.shape[1], trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Glove Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 250, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 125005    \n",
      "=================================================================\n",
      "Total params: 5,125,005\n",
      "Trainable params: 125,005\n",
      "Non-trainable params: 5,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.name=\"Glove Model\"\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model 4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 250, 100)          20000000  \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 250, 64)           6464      \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 80005     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,086,469\n",
      "Trainable params: 86,469\n",
      "Non-trainable params: 20,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.name=\"Model 4 Glove E\"\n",
    "model.add(e)\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(64))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model 5 Glove E\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 250, 100)          20000000  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 250, 55)           5555      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 125, 55)           0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 6875)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 250)               1719000   \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 5)                 1255      \n",
      "=================================================================\n",
      "Total params: 21,725,810\n",
      "Trainable params: 1,725,810\n",
      "Non-trainable params: 20,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.name=\"Model 5 Glove E\"\n",
    "model.add(e)\n",
    "model.add(Convolution1D(filters=55, kernel_size=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "#opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "opt = 'adam'\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the following cell one time only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_models_file = open('/Users/andrealequin/Desktop/Stage_NLP/Simple_extraction/accuracy_models_file.txt', 'w')\n",
    "#accuracy_models_file.write('fichier crÃ©Ã©')\n",
    "#accuracy_models_file.close()\n",
    "\n",
    "#accuracy_parameters_file = open('/Users/andrealequin/Desktop/Stage_NLP/Simple_extraction/accuracy_parameters_file.txt', 'w')\n",
    "#accuracy_parameters_file.write('num_expe test_acc val_acc train_acc epoch batch_size embed_dim optimizer learning_rate init_mode regularization regul_param max_nb_word max_seq_length\\n')\n",
    "#accuracy_parameters_file.close()\n",
    "\n",
    "num_expe = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 7\n",
      "Train on 9110 samples, validate on 1785 samples\n",
      "Epoch 1/1\n",
      "9110/9110 [==============================] - 4s 484us/step - loss: 0.0895 - accuracy: 0.9708 - val_loss: 0.6958 - val_accuracy: 0.8078\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 256\n",
    "print(model.name)\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_val, Y_val))#,callbacks=[EarlyStopping(monitor='val_loss', patience=20, min_delta=0.0001)])\n",
    "#accuracy_file = open('/Users/andrealequin/Desktop/Stage_NLP/Simple_extraction/accuracy_file.txt', 'w')\n",
    "#accuracy_file.write(str(model.summary()))\n",
    "#accuracy_file.close()\n",
    "\n",
    "# Open the file\n",
    "with open('/Users/andrealequin/Desktop/Stage_NLP/Simple_extraction/accuracy_models_file.txt','w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "    \n",
    "val_acc = str(history.history['val_accuracy'])\n",
    "train_acc = str(history.history['accuracy'])\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "test_acc = results[1]\n",
    "\n",
    "accuracy_parameters_file = open('/Users/andrealequin/Desktop/Stage_NLP/Simple_extraction/accuracy_models_file.txt', 'w')\n",
    "accuracy_parameters_file.write(str(num_expe)+' '+ test_acc +val_acc +train_acc epoch batch_size embed_dim optimizer learning_rate init_mode regularization regul_param max_nb_word max_seq_length\\n')\n",
    "accuracy_parameters_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [1 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 1 0 0 0]]\n",
      "[[1.3528421e-03 1.6186993e-03 2.9330724e-03 9.8596579e-01 8.1295492e-03]\n",
      " [6.4366329e-03 7.4076112e-03 3.0724794e-02 6.0917634e-01 3.4625465e-01]\n",
      " [9.9642700e-01 7.4894115e-04 1.7196660e-04 2.3224107e-03 3.2957166e-04]\n",
      " [9.9900609e-01 1.2309388e-04 1.5877011e-04 5.8633450e-04 1.2579457e-04]\n",
      " [3.6167121e-04 5.9294113e-04 7.2164985e-05 9.9885881e-01 1.1438524e-04]\n",
      " [3.4427838e-03 7.4790544e-03 1.1190693e-01 1.3553496e-02 8.6361778e-01]\n",
      " [1.1727656e-03 3.0610533e-04 9.9129301e-01 5.3895530e-03 1.8385195e-03]\n",
      " [2.3449161e-03 9.6384054e-01 2.2051315e-04 2.7292161e-03 3.0864786e-02]\n",
      " [1.0065999e-03 9.8953307e-01 7.6744836e-05 7.9820881e-04 8.5853152e-03]\n",
      " [9.7230133e-03 2.3949926e-03 9.4043946e-01 1.8279720e-02 2.9162794e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test[:10])\n",
    "predi = model.predict(X_test[:10])\n",
    "print(predi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 1 ... 3 3 3]\n",
      "[4 4 4 ... 3 3 3]\n",
      "[[338   8   1  39  14]\n",
      " [  8 346   2   7  37]\n",
      " [  3   7 108  42  25]\n",
      " [ 17   6  26 312  39]\n",
      " [ 13  48  21  52 266]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred_test = np.argmax(Y_pred, axis=1)\n",
    "print(y_pred)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "print(Y_test_int)\n",
    "print(confusion_matrix(Y_test_int, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911/911 [==============================] - 2s 2ms/step\n",
      "Test set\n",
      "  Loss: 2.092\n",
      "  Accuracy: 0.215\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d688d534588c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='val')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01,beta_1 = 0.9, momentum=0,init_mode='uniform',activation='relu',dropout_rate=0.0, weight_constraint=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "    #model.add(Dense(32, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(5, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    model.add(Dense(5, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    #optimizer = tf.keras.optimizers.SGD(lr=learn_rate, momentum=momentum)\n",
    "    #tf.keras.optimizers.SGD(lr=learn_rate, momentum=momentum)\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learn_rate, beta_1=beta_1)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=256, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "#momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "learn_rate = [0.1, 0.2]\n",
    "momentum = [0.4,0.8]\n",
    "beta_1 = [0.4, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(learn_rate=learn_rate, beta_1=beta_1)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3,return_train_score=True)\n",
    "grid_result = grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.226789 using {'beta_1': 0.4, 'learn_rate': 0.1}\n",
      "0.226789 0.215918 (0.034586) with: {'beta_1': 0.4, 'learn_rate': 0.1}\n",
      "0.219543 0.219540 (0.024341) with: {'beta_1': 0.4, 'learn_rate': 0.2}\n",
      "0.219543 0.219540 (0.024341) with: {'beta_1': 0.9, 'learn_rate': 0.1}\n",
      "0.219543 0.219540 (0.024341) with: {'beta_1': 0.9, 'learn_rate': 0.2}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "means_train = grid_result.cv_results_['mean_train_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, mean_train, stdev, param in zip(means,means_train, stds, params):\n",
    "    print(\"%f %f (%f) with: %r\" % (mean, mean_train, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model 4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 250, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 250, 64)           6464      \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 5)                 80005     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 5,086,469\n",
      "Trainable params: 5,086,469\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.name=\"Model 4\"\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(64))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
